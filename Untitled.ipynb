{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Insincere Questions on Quora\n",
    "\n",
    "This blog post is about the challenge that is hosted on kaggle [Quora Insincere Questions](https://www.kaggle.com/c/quora-insincere-questions-classification/overview). \n",
    "\n",
    "This post is divided into five parts:\n",
    "\n",
    "1. Overview\n",
    "2. Evaluation Metrics\n",
    "3. Basic EDA and Data Preprocessing\n",
    "4. Base Line Model\n",
    "5. Deep Learning Models\n",
    "6. Conclusion\n",
    "\n",
    "Lets get started!\n",
    "\n",
    "![GIF](https://media1.tenor.com/images/7dcc0b5a2c64d741b6edd12a88738cf9/tenor.gif?itemid=4767352)\n",
    "\n",
    "### 1. Overview:\n",
    "Quora is a platform that empowers people to learn from each other. One can go to Quora and ask their questions and get answers from others. But some questions asked by users may be toxic and contain divisive content. The aim of the competition is to tackle these situations.\n",
    "\n",
    "The dataset is a csv file you can download it from [here](https://www.kaggle.com/c/quora-insincere-questions-classification/data).\n",
    "\n",
    "<b>qid</b>: Question ID\n",
    "\n",
    "<b>question_text</b>:  Question text.\n",
    "\n",
    "<b>target</b>: target whether the question is sincere or not. if question is insincere then target is 1 else 0.\n",
    "\n",
    "\n",
    "### 2. Evaluation Metrics\n",
    "F1-Score = 2 x (precision x recall) / (precision + recall)\n",
    "\n",
    "<b>Precision</b>: The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "<b>Recall</b>: The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "### 3. Basic EDA and Data Preprocessing\n",
    "```python\n",
    "# load the dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "```\n",
    "<img src=\"assets/dataset.png\"/>\n",
    "\n",
    "#### 3.a Target Label Distribution\n",
    "\n",
    "<img src=\"assets/distribution.png\"/>\n",
    "\n",
    "The above plot clearly shows that the dataset is a imbalanced dataset with over 12,25,312 questions are sincere with target as 0 and 80810 are marked as insincere with target label as 1\n",
    "\n",
    "#### 3.b Text Analysis\n",
    "\n",
    "Finding the length of each question text in dataset\n",
    "\n",
    "```python\n",
    "# calculate length of question text\n",
    "train_df['length'] = train_df['question_text'].apply(lambda x: len(x))\n",
    "```\n",
    "\n",
    "```python\n",
    "print(\"Question text\")\n",
    "print('='*25)\n",
    "print(x['question_text'].values[0])\n",
    "print('='*25)\n",
    "print(\"Target for this question:\", x['target'].values[0])\n",
    "```\n",
    "<img src=\"assets/max_len.png\"/>\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "This question is has lot of math operations like powers mulitplications and all and the interesting part is question has been marked as insincere.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "```python\n",
    "x = train_df[train_df['length'] == train_df['length'].min()]\n",
    "print(\"Question text\")\n",
    "print('='*25)\n",
    "print(x['question_text'].values[0])\n",
    "print('='*25)\n",
    "print(\"Target for this question:\", x['target'].values[0])\n",
    "```\n",
    "\n",
    "<img src=\"assets/min_len.png\"/>\n",
    "\n",
    "<br/>\n",
    "This question has no text. Interestingly this has been marked as <b>insincere</b>. since there is no point of keeping this point in the dataset we can remove this.\n",
    "<br/>\n",
    "\n",
    "Lets check distribution of length feature for sincere and insincere questions.\n",
    "\n",
    "\n",
    "<img src=\"assets/length_dist.png\"/>\n",
    "\n",
    "Length seems to be not much useful from the above plot.\n",
    "\n",
    "\n",
    "#### Lets check bad words count in a question\n",
    "\n",
    "You can find the list of words in this repo https://github.com/RobertJGabriel/Google-profanity-words/blob/master/list.txt\n",
    "\n",
    "```python\n",
    "bad_words_df = pd.read_csv('bad_words_list.txt')\n",
    "bad_words_df.head()\n",
    "```\n",
    "<img src=\"assets/bad_words_df.png\"/>\n",
    "\n",
    "Code for extracting bad words from question text\n",
    "\n",
    "```python\n",
    "# extract bad words from question text\n",
    "bad_words = list(bad_words_df['word'].values)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_bad_words(question_texts):\n",
    "    b_count = []\n",
    "    for question in tqdm(question_texts):\n",
    "        b_count.append(sum([1 for w in bad_words if w.lower() in question.lower()]))\n",
    "    return b_count\n",
    "\n",
    "train_df['bad_words_count'] = count_bad_words(train_df['question_text'].values)\n",
    "\n",
    "print(train_df['bad_words_count'].min(), train_df['bad_words_count'].max())\n",
    "```\n",
    "\n",
    "Output : 0, 9\n",
    "\n",
    "Distribution of bad_words_count for sincere and insincere questions\n",
    "\n",
    "<img src=\"assets/bad_words_dist.png\"/>\n",
    "\n",
    "Looks like this feature is not much useful.\n",
    "\n",
    "Similarly we can find many other features like Country Count, Sentence Count, Word Count, Stop Word Count, Punctuation count, Average word length and code for all these feature is available in this [notebook](notebooks/QuoraInsincereQuestions-EDA.ipynb) but none of these features found helpful.\n",
    "\n",
    "#### 3.c Text Preprocessing\n",
    "\n",
    "We will be using [GloVe](https://nlp.stanford.edu/projects/glove/) for obtaining vector representation of a word as this is useful for our Deep learning model which will be covered at the end.\n",
    "\n",
    "Code for loading the glove vector representations\n",
    "\n",
    "```python\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embed(file):    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index\n",
    "\n",
    "%%time\n",
    "embed_glove = load_embed('glove.840B.300d.txt')\n",
    "```\n",
    "Once we have loaded Glove vector representations into dictionary we can access vector representations of any word by embeddings_index[word]. So embeddings_index is generally a dictionary containing key as a word and its value is a vector representation.\n",
    "\n",
    "```python\n",
    "\n",
    "def build_vocab(texts):\n",
    "    \"\"\"\n",
    "    Helper function to create a vocab count for our data\n",
    "    \"\"\"\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    \"\"\"\n",
    "    Helper function to check the code coverage with embedding matrix\n",
    "    \"\"\"\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word.strip()] = embeddings_index[word.strip()]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "```\n",
    "\n",
    "So lets start by checking code coverage for our raw question text'\n",
    "\n",
    "```python\n",
    "%%time\n",
    "vocab = build_vocab(train_df['question_text'])\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "```\n",
    "Output<br/>\n",
    "\n",
    "Glove : <br/>\n",
    "Found embeddings for 33.02% of vocab<br/>\n",
    "Found embeddings for  88.15% of all text<br/>\n",
    "\n",
    "We can only get vector representations for 33% of vocab(unique words) and 88.15% of all text.\n",
    "\n",
    "Lets check what are missing in this\n",
    "\n",
    "```python\n",
    "print(oov_glove[:10])\n",
    "```\n",
    "Output:\n",
    "\n",
    "[('India?', 16384),\n",
    " ('it?', 12900),\n",
    " (\"What's\", 12425),\n",
    " ('do?', 8753),\n",
    " ('life?', 7753),\n",
    " ('you?', 6295),\n",
    " ('me?', 6202),\n",
    " ('them?', 6140),\n",
    " ('time?', 5716),\n",
    " ('world?', 5386)]\n",
    " \n",
    " \n",
    "As we can see from the above cell lot of text processing needs to be done. India? has appeared 16,384 times in our question texts.\n",
    "\n",
    "Next step is to clean the raw questions by lower casing the text, expanding contractions and removing punctuations and check if our we can increase our vocab coverage.\n",
    "\n",
    "```python\n",
    "train_df['lowered_question'] = train_df['question_text'].apply(lambda x: x.lower())\n",
    "# expanding code contractions \n",
    "# refer to https://gist.github.com/nealrs/96342d8231b75cf4bb82\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "train_df['cleaned_text'] = train_df['lowered_question'].apply(lambda x: expandContractions(x))\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', \n",
    "          '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', \n",
    "          '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢',\n",
    "          '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕',\n",
    "          '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', \n",
    "          '∞','∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', \n",
    "          'ï', 'Ø', '¹', '≤', '‡', '√', '∞', 'θ', '÷', 'α', '•', 'à', '−', 'β', '∅', '³', 'π', '‘','₹', \n",
    "          '´', \"'\", '°', '£', '€', '×', '™','√','²','—–','&','…', \"’\", \"“\", \"”\", \"#\", \"{\", \"|\", \"}\", \"~\"]\n",
    "          \n",
    "def remove_punctuations(text):\n",
    "    for p in unknown_punctuations:\n",
    "        text = text.replace(p, ' ')\n",
    "    for p in known_puncts:\n",
    "        text = text.replace(p, ' ' + p + ' ')\n",
    "    return text\n",
    "    \n",
    "train_df['final_cleaned_text'] = train_df['cleaned_text'].apply(lambda x: remove_punctuations(x))\n",
    "\n",
    "vocab = build_vocab(train_df['final_cleaned_text'])\n",
    "\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "\n",
    "```\n",
    "Output:\n",
    "\n",
    "Glove : <br/>\n",
    "Found embeddings for 69.57% of vocab<br/>\n",
    "Found embeddings for  99.58% of all text<br/>\n",
    "\n",
    "Now the code coverage is increased from 33% to 69.57% and covers 99.58% of all text. So removing punctuations, lower casing and expanding contractions increased vocab coverage.\n",
    "\n",
    "We can further increase the coverage by corecting the spellings as there are lot of spelling mistakes in question text. You can find detailed code on this [notebook](notebooks/QuoraInsincereQuestions-Text-Preprocessing.ipynb)   \n",
    "\n",
    "\n",
    "### 4. Base Line Model\n",
    "\n",
    "We will be using Naive Bayes as our base line model with TFIDF vectorized words as Naive Bayes works pretty fast and can be used as base line model for many projects.\n",
    "\n",
    "Complete code can be found in this [notebook](notebooks/QuoraInsincereQuestions-ML-Models.ipynb)\n",
    "```python\n",
    "%%time\n",
    "# Estimator as MultinomialNB for GridSearchCV\n",
    "mnb = MultinomialNB()\n",
    "# Estimator MultinomialNB\n",
    "# param_grid: Additive (Laplace/Lidstone) smoothing parameter : (10^(-10) to 10^(10))\n",
    "# cv: Cross validation with 3 fold\n",
    "# scoring: perform cross validation based on f1\n",
    "# verbose: for debugging\n",
    "clf = GridSearchCV(estimator=mnb, param_grid={'alpha': list(map(lambda x: 10**x , range(-10,10))) }, cv=3,\n",
    "                   scoring='f1',verbose=10, n_jobs=-1)\n",
    "# Fit the train samples\n",
    "clf.fit(tfidf_train_vect, y_train.values)\n",
    "\n",
    "print(\"TFIDF train f1 score:\", f1_score(y_pred=clf.best_estimator_.predict(tfidf_train_vect), \\\n",
    "                                      y_true=y_train.values))\n",
    "print(\"TFIDF test f1 score:\", f1_score(y_pred=clf.best_estimator_.predict(tfidf_test_vect), \\\n",
    "                                     y_true=y_test.values))\n",
    "                                     \n",
    "```\n",
    "Output:\n",
    "\n",
    "TFIDF train f1 score: 0.7238521247204315<br/>\n",
    "TFIDF test f1 score: 0.5829645584491874\n",
    "\n",
    "By using Naive Bayes Classifier we are able to get 0.583 F1-score on our test set as this a pretty good score we can use it as base line model.\n",
    "\n",
    "### 5. Deep Learning Model\n",
    "\n",
    "We will be using Bidirectional LSTM with a [Attention Layer]() folowed by a dense layer of 64 units and another dense layer of 32 units with [elu](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu) activation function for our model. Attention Layer is based on the Dzmitry Bahdanau research paper https://arxiv.org/pdf/1409.0473.pdf and custom keras implementation is taken from kaggle kernel https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043 and detailed explaination will be found in this [notebook](notebooks/AttentionLayerDemo.ipynb), in this notebook there is a detailed explanation of input shapes and output shapes at of each operation used for the implmentation.\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n",
    "model.add(Attention(MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[get_f1])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "<img src='assets/model.png'/>\n",
    "\n",
    "```python\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=2048*2,\n",
    "          epochs=25,\n",
    "          validation_split=0.3, callbacks=callbacks_list)\n",
    "score, acc = model.evaluate(x_val, y_val,\n",
    "                            batch_size=2048)\n",
    "\n",
    "print('Test Loss:', score)\n",
    "print('Test F1 Score:', acc)\n",
    "```\n",
    "Output:\n",
    "\n",
    "391836/391836 [==============================] - 30s 77us/step<br/>\n",
    "Test Loss: 0.10312879059361556<br/>\n",
    "Test F1 Score: 0.6746514823591312<br/>\n",
    "\n",
    "Lets check the model loss plot on train and validation sets\n",
    "\n",
    "<img src='assets/loss.png'/>\n",
    "\n",
    "\n",
    "After 7 epochs the training loss has decreased constantly but the validation loss does not decrease much and fluctuated from 0.11 to 0.12 . So we keep our checkpoint of the at epoch 7 and save the model.\n",
    "\n",
    "Lets check the model f1-score plot on train and validation sets\n",
    "\n",
    "<img src='assets/f1score.png'/>\n",
    "\n",
    "From the abpve figure by looking at the f1 score after 7th epoch validation set f1 score became amost constant has no significant improvement and training set f1 score increased indicating the model is overfitting.\n",
    "\n",
    "\n",
    "### 6. Conclusion\n",
    "\n",
    "With bidirectional lstm using attention layer we are able to get 0.675 f1 score on our test set. We can improve the score by doing more text processing fixing the spell corrections and handling symbols and other special characters, adding more LSTM layers etc.\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. https://www.kaggle.com/c/quora-insincere-questions-classification\n",
    "2. https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course\n",
    "3. keras Attention layer implentation https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "4. https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "5. https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
